---
layout: post
title: "坐井说天阔---DeepSeek-R1"
date: 2025-02-15
categories: [AI, 深度学习]
tags: [AI, DeepSeek, 大模型, 强化学习]
author: 王凯
excerpt: "作者在文中的结论有一定的余地，得出来两个结论。首先将更强大的模型蒸馏成更小的模型会产生极好的结果，但小模型进行依赖于本大规模RL训练需要巨大的计算能力，甚至可能无法达到蒸馏的性能。"
---

> 原文链接：[https://blog.csdn.net/weixin_42754905/article/details/145648999](https://blog.csdn.net/weixin_42754905/article/details/145648999)

# 前言

DeepSeek-R1是DeepSeek团队发布的推理模型。本文将深入分析DeepSeek-R1的技术细节，包括强化学习训练、模型蒸馏等关键技术。

# DeepSeek-R1-Zero, 纯强化学习训练

虽然这个尝试不尽人意，但其训练过程还是具有一定参考意义。也是实验过程中发现了两件有意思的事情。

首先训练过程中，发现随着测试时间的增加，DeepSeek-R1-Zero出现了自我进化的现象，会进行自我进行更复杂的行为，如下图所示，比如会主动重新评估先前的步骤，会自己探索解决方案的代替方法。这些行为是与强化学习环境交互自己出现的，并且显著增强了模型的推理能力，从而使得训练出来的模型可以更高效，更准确的处理更具有挑战性的任务。

![DeepSeek-R1-Zero average length per response during training](https://img-blog.csdnimg.cn/direct/xxx.png)

另外一件事是"Aha Moment"，DeepSeek-R1-Zero 自己会分配更多的思考时间来重新评估其初始方法，从而思考是否有更有效，更准确的解决方案。下图给了一个具体的例子。

"Aha Moment"说明了，对于大模型推理能力的训练，不需要明确的教它如何解决问题，只需要简单地为其提供正确的激励措施，然后它就会自主进行思考从而找到更有效的解决策略。"Aha Moment"也在提醒我们，强化学习有可能在AI训练中解锁新的智能水平，为未来开发出更加自主和自适应的大模型铺平道路。

## Aha Moment 示例

**Question:** If $a > 1$, then the sum of the real solutions of $\sqrt{a - \sqrt{a + x}} = x$ is equal to

**Response:** 

```
<think>
To solve the equation $\sqrt{a - \sqrt{a + x}} = x$, let's start by squaring both $\cdots$

$$
\left(\sqrt{a - \sqrt{a + x}}\right)^{2} = x^{2} \Longrightarrow a - \sqrt{a + x} = x^{2}.
$$

Rearrange to isolate the inner square root term:

$$
(a - x^{2})^{2} = a + x \Longrightarrow a^{2} - 2ax^{2} + (x^{2})^{2} = a + x \Longrightarrow x^{4} - 2ax^{2} - x + (a^{2} - a) = 0
$$

...

Wait, wait. Wait. That's an aha moment I can flag here.

Let's reevaluate this step-by-step to identify if the correct sum can be $\cdots$

We started with the equation:

$$
\sqrt{a - \sqrt{a + x}} = x
$$

First, let's square both sides:

$$
a - \sqrt{a + x} = x^{2} \Longrightarrow \sqrt{a + x} = a - x^{2}
$$

Next, I could square both sides again, treating the equation: $\cdots$

...
</think>
```

# DeepSeek-R1, 使用冷启动进行强化学习

DeepSeek-R1-Zero 遇到了可读性差和语言混合等挑战。为了解决DeepSeek-R1-Zero遇到的问题，从两个方向进行了尝试：

1. 是不是可以使用少量高质量的数据作为冷启动（cold start）来进一步提高推理性能或者减少训练成本
2. 对于DeepSeek-R1-Zero 遇到的可读性差和语言混合等挑战，如何训练出一个模型。用户友好，可以产生清晰连贯的思维链（CoT），也有强大的通用能力。

为了获取高质量的数据，从以下三种方式进行了数据生产，论文中称之为cold start data（冷启动数据）：

1. 用具有长思维链（CoT）的少样本（few-shot）提示词
2. 直接让模型来通过思考和验证来生成详细的答案
3. 收集DeepSeek-R1-Zero的输出，然后通过人工标注进行后处理来提炼结果

有了冷启动数据之后便开始微调 DeepSeek-V3-Base 模型。结果显示可读性大大增强。而且相对于DeepSeek-R1-Zero，使用人类设计标注的冷启动数据的模型在推理benchmark和通用能力benchmark更加均衡。

接着继续在冷启动数据基础上进行RL（强化学习）训练，观察到在推理和通用任务方面都有进步，但改善不够理想。于是又增加了验证过程来强化推理能力，第二轮RL之后，在推理任务上取得重大进步，比如 AIME 2024（pass@1）从 39.2% 增加到 79.8%。

然而，经过第二轮 RL 之后，在通用任务中有一些退化，为了解决这个问题，又进行第三轮的强化学习训练。在第三轮 RL 中，训练数据集扩展到包括更多元化的通用任务和推理相关查询。这些改进提高了推理能力，并最大限度地减少了通用能力的退化。最终，DeepSeek-R1 在推理和通用 benchmark 上均表现出色。

此外，在前两轮的 RL 中，主要侧重于提高助手的有用性。然后将其扩展到具有 safety RL的任务，以确保模型对有害查询产生有用且安全的响应。尽管它提高了安全性，但稍微降低了准确性。

具体来说，AIME 2024（pass@1）从 79.8% 降至 71.0%，GPQA Diamond 从 71.5% 降至 59.1%。DeepSeek-R1在安全 RL 之后比 DeepSeek-R1（w/o safety）在 SimpleSafetyTests 上稍差，主要是因为它倾向于在安全 RL 之后拒绝回答某些查询。在没有安全 RL 的情况下，DeepSeek-R1 可以达到超过 70% 的准确率。

# Distillation: Empower Small Models with Reasoning Capability (蒸馏：为小模型赋予推理能力)

此外也将收集到的 800k 样本直接对 Qwen 和 Llama 等模型进行了微调，这种简单的蒸馏方式显著增强了模型的推理能力。蒸馏模型评估结果如下，蒸馏法具有强大的潜力。

## 蒸馏模型评估结果

| Model | AIME 2024 |   | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |
|-------|-----------|---|----------|--------------|----------------|------------|
|       | pass@1    | cons@64 | pass@1 | pass@1 | pass@1 | rating |
| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 | 759 |
| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 | 717 |
| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 | 1820 |
| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 | 1316 |
| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 | 954 |
| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 | 1189 |
| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 | 1481 |
| DeepSeek-R1-Distill-Qwen-32B | 72.6 | 83.3 | 94.3 | 62.1 | 57.2 | 1691 |
| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 | 1205 |
| DeepSeek-R1-Distill-Llama-70B | 70.0 | 86.7 | 94.5 | 65.2 | 57.5 | 1633 |

**Table 5 |** Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.

在蒸馏过程中，只使用了SFT（Supervised Fine-Tuning）数据进行微调。为了验证大规模 RL 训练实现与蒸馏相当的性能，在这一节又进行了一步实验，在Qwen-32B模型上进行大规模的0样本强化训练，结果显示，不如使用SFT（Supervised Fine-Tuning）数据进行微调的结果。

## 蒸馏与RL模型对比

| Model | AIME 2024 |   | MATH-500 | GPQA Diamond | LiveCodeBench |
|-------|-----------|---|----------|--------------|---------------|
|       | pass@1    | cons@64 | pass@1 | pass@1 | pass@1 |
| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |
| DeepSeek-R1-Zero-Qwen-32B | 47.0 | 60.0 | 91.6 | 55.0 | 40.2 |
| DeepSeek-R1-Distill-Qwen-32B | 72.6 | 83.3 | 94.3 | 62.1 | 57.2 |

**Table 6 |** Comparison of distilled and RL Models on Reasoning-Related Benchmarks.

# 总结和展望

作者在文中的结论有一定的余地，得出来两个结论。

首先将更强大的模型蒸馏成更小的模型会产生极好的结果，但小模型进行依赖于本大规模RL训练需要巨大的计算能力，甚至可能无法达到蒸馏的性能。

其次，虽然蒸馏策略既经济又有效，但超越intelligence界限可能仍然需要更强大的基础模型和更大规模的强化学习。

对未来有以下三点展望：

- **通用能力**：目前，DeepSeek-R1 在函数调用、多轮次、复杂角色扮演和 JSON 输出等任务方面的能力不如 DeepSeek-V3。展望未来，我们计划探索可以利用 CoT 来增强这些领域的任务。
  
- **语言混合**：DeepSeek-R1 目前针对中文和英文进行了优化，这可能会导致在处理其他语言的查询时出现语言混合问题。例如，DeepSeek-R1 可能会使用英语进行推理和响应，即使查询使用的是英语或中文以外的语言。我们的目标是在将来的更新中解决此限制。
  
- **提示工程**：在评估 DeepSeek-R1 时，我们观察到它对提示很敏感。Few-shot 提示始终会降低其性能。因此，我们建议用户直接描述问题并使用zero-shot设置指定输出格式以获得最佳结果。

# ps

膜拜大佬。DeepSeek-R1虽然不是第一个推理大模型，但R1的出现第一次让推理模型能被大多数人用上，而且是免费用上，并且进行了开源。此外，蒸馏小模型的尝试，也提供了一种新的思路--通过少样本在通用大模型基础上进行微调，同样可以实现具有推理能力的推理模型。这对于企业训练微调自己的推理模型，提供了技术可行性和经济可行性。

---

**参考链接**：
- [DeepSeek-R1 技术报告](https://github.com/deepseek-ai/DeepSeek-R1)
- [原文CSDN博客](https://blog.csdn.net/weixin_42754905/article/details/145648999)
